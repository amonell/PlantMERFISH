{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import bigfish\n",
    "import bigfish.stack as stack\n",
    "import bigfish.detection as detection\n",
    "import bigfish.multistack as multistack\n",
    "import bigfish.plot as plot\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pickle\n",
    "import glob\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and pad all MERSCOPE smFISH images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_imgs = r'D:\\Tatsuya\\merscope\\202210221316_avrrpt2-9h-rep1_VMSC01101\\out\\images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_to_imgs, 'mosaic_BacMeta_z3.tif')\n",
    "an_image_bac = io.imread(file_path)\n",
    "bac_image_padded = np.pad(an_image_bac, pad_width=((0, 2000 - np.shape(an_image_bac)[0]%2000), (0,0)))\n",
    "bac_image_padded = np.pad(bac_image_padded, pad_width=((0, 0), (0,2000 - np.shape(an_image_bac)[1]%2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =  os.path.join(path_to_imgs, 'mosaic_DAPI_z3.tif')\n",
    "an_image_dapi = io.imread(file_path)\n",
    "dapi_image_padded = np.pad(an_image_dapi, pad_width=((0, 2000 - np.shape(an_image_dapi)[0]%2000), (0,0)))\n",
    "dapi_image_padded = np.pad(dapi_image_padded, pad_width=((0, 0), (0,2000 - np.shape(an_image_dapi)[1]%2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_to_imgs, 'mosaic_sid_z3.tif')\n",
    "an_image_sid = io.imread(file_path)\n",
    "sid_image_padded = np.pad(an_image_sid, pad_width=((0, 2000 - np.shape(an_image_sid)[0]%2000), (0,0)))\n",
    "sid_image_padded = np.pad(sid_image_padded, pad_width=((0, 0), (0,2000 - np.shape(an_image_sid)[1]%2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_to_imgs, 'mosaic_hrpl_z3.tif')\n",
    "an_image_hrpl = io.imread(file_path)\n",
    "hrpl_image_padded = np.pad(an_image_hrpl, pad_width=((0, 2000 - np.shape(an_image_hrpl)[0]%2000), (0,0)))\n",
    "hrpl_image_padded = np.pad(hrpl_image_padded, pad_width=((0, 0), (0,2000 - np.shape(an_image_hrpl)[1]%2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(path_to_imgs, 'mosaic_pvds_z3.tif')\n",
    "an_image_pvds = io.imread(file_path)\n",
    "pvds_image_padded = np.pad(an_image_pvds, pad_width=((0, 2000 - np.shape(an_image_pvds)[0]%2000), (0,0)))\n",
    "pvds_image_padded = np.pad(pvds_image_padded, pad_width=((0, 0), (0,2000 - np.shape(an_image_pvds)[1]%2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r'D:\\Alex\\MERSCOPE_reanalysis_output\\smFISH_output\\avr_9hr'\n",
    "os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantify spots across all images (Hyperparameters will need tuning if using a different set of images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything will get saved to the outfolder. Read BigFISH documentation to choose the correct hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import overload\n",
    "from sklearn import cluster\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "over_counter = 0\n",
    "for i_o in range(2000, len(bac_image_padded[0]), 2000):\n",
    "    for j_o in range(2000, len(bac_image_padded), 2000): \n",
    "        iamge_play = bac_image_padded[i_o-2000:i_o, j_o-2000:j_o]\n",
    "        iamge_play_dapi = dapi_image_padded[i_o-2000:i_o, j_o-2000:j_o]\n",
    "        iamge_play_sid = sid_image_padded[i_o-2000:i_o, j_o-2000:j_o]\n",
    "        iamge_play_hrpl = hrpl_image_padded[i_o-2000:i_o, j_o-2000:j_o]\n",
    "        iamge_play_pvds = pvds_image_padded[i_o-2000:i_o, j_o-2000:j_o]\n",
    "#         spots, threshold = detection.detect_spots(  \n",
    "#             images=iamge_play, \n",
    "#             threshold=95,\n",
    "#             return_threshold=True, \n",
    "#             spot_radius=(17, 17),\n",
    "#             voxel_size = (3, 3))   \n",
    "        spots, threshold = detection.detect_spots(  \n",
    "            images=iamge_play, \n",
    "            threshold=200,\n",
    "            return_threshold=True, \n",
    "            log_kernel_size=(1.456, 1.456),\n",
    "            minimum_distance=(1.456, 1.456))\n",
    "        spots_dapi = np.array([])\n",
    "        spots_sid = np.array([])\n",
    "        spots_hrpl = np.array([])\n",
    "        spots_pvds = np.array([])\n",
    "        spots_dapi, threshold_dapi = detection.detect_spots(  \n",
    "            images=iamge_play_dapi, \n",
    "            return_threshold=True, \n",
    "            threshold=180,\n",
    "            log_kernel_size=(1.456, 1.456),\n",
    "            minimum_distance=(1.456, 1.456))\n",
    "\n",
    "        spots_sid, threshold_sid = detection.detect_spots(  \n",
    "            images=iamge_play_sid, \n",
    "            return_threshold=True,\n",
    "            threshold=1000,\n",
    "            log_kernel_size=(1.456, 1.456),\n",
    "            minimum_distance=(1.456, 1.456))\n",
    "        \n",
    "        spots_hrpl, threshold_sid = detection.detect_spots(  \n",
    "            images=iamge_play_hrpl, \n",
    "            return_threshold=True,\n",
    "            threshold=200,\n",
    "            log_kernel_size=(1.456, 1.456),\n",
    "            minimum_distance=(1.456, 1.456))\n",
    "\n",
    "        spots_pvds, threshold_sid = detection.detect_spots(  \n",
    "            images=iamge_play_pvds, \n",
    "            return_threshold=True,\n",
    "            threshold=300,\n",
    "            log_kernel_size=(1.456, 1.456),\n",
    "            minimum_distance=(1.456, 1.456))\n",
    "\n",
    "        total_spots = list(spots_dapi)\n",
    "        type_spots = ['dapi' for i in range(len(spots_dapi))]\n",
    "        for i in spots:\n",
    "            total_spots.append(i)\n",
    "            type_spots.append('bac')\n",
    "        for i in spots_sid:\n",
    "            total_spots.append(i)\n",
    "            type_spots.append('sid')\n",
    "\n",
    "        total_spots = np.array(total_spots)\n",
    "        if len(total_spots) > 0:\n",
    "            clustering = DBSCAN(eps = 35, min_samples=5).fit(total_spots)\n",
    "            cluster_types = np.unique(clustering.labels_)\n",
    "            if len(cluster_types) != 1:\n",
    "                for i in cluster_types:\n",
    "                    if i != -1:\n",
    "                        locations_cluster = np.where(clustering.labels_ == i)\n",
    "                        counts_labels = np.unique(np.array(type_spots)[locations_cluster], return_counts=True)\n",
    "                        counts_dapi = counts_labels[1][np.where(counts_labels[0] == 'dapi')]\n",
    "                        counts_bac = counts_labels[1][np.where(counts_labels[0] == 'bac')]\n",
    "                        counts_sid = counts_labels[1][np.where(counts_labels[0] == 'sid')]\n",
    "                        if len(counts_bac) == 0:\n",
    "                            counts_bac = np.array([0])\n",
    "                        if len(counts_dapi) == 0:\n",
    "                            counts_dapi = np.array([0])\n",
    "                        if len(counts_sid) == 0:\n",
    "                            counts_sid = np.array([0])\n",
    "                        if (counts_dapi+counts_sid)/(counts_dapi+counts_sid+counts_bac) > 0.3:\n",
    "                            clustering.labels_[locations_cluster] = -1\n",
    "\n",
    "            cl_new, index_types = np.unique(clustering.labels_, return_index=True)\n",
    "            import numpy as np\n",
    "            import scipy as sp\n",
    "\n",
    "            def dbscan_predict(dbscan_model, X_new, metric=sp.spatial.distance.cosine):\n",
    "                # Result is noise by default\n",
    "                y_new = np.ones(shape=len(X_new), dtype=int)*-1 \n",
    "\n",
    "                # Iterate all input samples for a label\n",
    "                for j, x_new in enumerate(X_new):\n",
    "                    # Find a core sample closer than EPS\n",
    "                    for i, x_core in enumerate(dbscan_model.components_): \n",
    "                        if metric(x_new, x_core) < dbscan_model.eps:\n",
    "                            # Assign label of x_core to x_new\n",
    "                            y_new[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]\n",
    "                            break\n",
    "\n",
    "                return y_new\n",
    "            x_new = [i for i in spots_hrpl]\n",
    "            for i in spots_pvds:\n",
    "                x_new.append(i)\n",
    "            figure, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 10))\n",
    "            ax1.imshow(iamge_play)\n",
    "            ax1.scatter([i[1] for i in spots], [i[0] for i in spots], c='red', s=1, label = 'Bacterial Genes')\n",
    "            ax1.scatter([i[1] for i in spots_dapi], [i[0] for i in spots_dapi], c='yellow', s=1, label = 'DAPI')\n",
    "            ax1.scatter([i[1] for i in spots_sid], [i[0] for i in spots_sid], c='green', s=1, label = 'SID')\n",
    "            ax1.scatter([i[1] for i in spots_hrpl], [i[0] for i in spots_hrpl], c='blue', s=1, label = 'hrpL')\n",
    "            ax1.scatter([i[1] for i in spots_pvds], [i[0] for i in spots_pvds], c='purple', s=1, label = 'pvds')\n",
    "            ax1.set_title('Bacterial Genes and Dapi Overlay')\n",
    "            ax1.axis('off')\n",
    "            ax2.imshow(iamge_play)\n",
    "            ax2.scatter([i[1] for i in total_spots], [i[0] for i in total_spots], c=clustering.labels_, s=1)\n",
    "            \n",
    "            \n",
    "            ax3.imshow(iamge_play_dapi)\n",
    "            ax3.set_title('SID image')\n",
    "            ax3.axis('off')\n",
    "            for i in range(len(cl_new)):\n",
    "                if cl_new[i] != -1:\n",
    "                    ax2.annotate(cl_new[i], (total_spots[index_types[i]][1], total_spots[index_types[i]][0]), color='white')\n",
    "            ax2.set_title('Candidate Bacterial Genes')\n",
    "            ax2.axis('off')\n",
    "            try:\n",
    "                os.mkdir(os.path.join(output_folder, 'pics'))\n",
    "                os.mkdir(os.path.join(output_folder, 'total_spots'))\n",
    "                os.mkdir(os.path.join(output_folder, 'type_spots'))\n",
    "                os.mkdir(os.path.join(output_folder, 'hrpl_spots'))\n",
    "                os.mkdir(os.path.join(output_folder, 'pvds_spots'))\n",
    "                os.mkdir(os.path.join(output_folder, 'dbscan'))\n",
    "            except:\n",
    "                None\n",
    "            plt.savefig(os.path.join(output_folder, 'pics', 'pic'+str(i_o)+'_'+str(j_o)+'.png'))\n",
    "            plt.close()\n",
    "        \n",
    "            np.save(os.path.join(output_folder, 'total_spots', 'total_spots'+str(i_o)+'_'+str(j_o)+'.npy'), total_spots)\n",
    "            np.save(os.path.join(output_folder, 'type_spots', 'type_spots'+str(i_o)+'_'+str(j_o)+'.npy'), type_spots)\n",
    "            np.save(os.path.join(output_folder, 'hrpl_spots', 'hrpl_spots'+str(i_o)+'_'+str(j_o)+'.npy'), spots_hrpl)\n",
    "            np.save(os.path.join(output_folder, 'pvds_spots', 'pvds_spots'+str(i_o)+'_'+str(j_o)+'.npy'), spots_pvds)\n",
    "            \n",
    "            file = open(os.path.join(output_folder, 'dbscan', 'dbscan'+str(i_o)+'_'+str(j_o)+'.pkl'), 'wb')\n",
    "            # dump information to that file\n",
    "            pickle.dump(clustering, file)\n",
    "            file.close()\n",
    "\n",
    "\n",
    "        over_counter += 1\n",
    "        print(over_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use DBscan to find bacterial colonies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = output_folder\n",
    "dbscans = glob.glob(os.path.join(path, 'dbscan', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fov = []\n",
    "total_centroids = []\n",
    "total_cluster_components = []\n",
    "total_labels = []\n",
    "for db in dbscans:\n",
    "    with open(db, 'rb') as f:\n",
    "        db_contents = pickle.load(f)\n",
    "    f.close()\n",
    "    total_spots_file = db.replace('dbscan', 'total_spots').replace('.pkl', '.npy')\n",
    "    total_spots_array = np.load(total_spots_file)\n",
    "    fov.append(os.path.basename(db))\n",
    "    centroids = []\n",
    "    all_components = []\n",
    "    for lab in np.unique(db_contents.labels_):\n",
    "        if lab != -1:\n",
    "            components_cluster = total_spots_array[np.where(db_contents.labels_ == lab)]\n",
    "            centroid = np.mean(components_cluster, axis = 0)\n",
    "            centroids.append(centroid)\n",
    "            all_components.append(components_cluster)\n",
    "    total_centroids.append(centroids)\n",
    "    total_cluster_components.append(all_components)\n",
    "    total_labels.append(db_contents.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_nums = [np.array(i.replace('dbscan', '').replace('.pkl', '').split('_')).astype(int) for i in fov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval_positions = []\n",
    "yval_positions = []\n",
    "for i in range(len(total_centroids)):\n",
    "    xval_cent = []\n",
    "    yval_cent = []\n",
    "    if len(total_centroids[i]) > 0:\n",
    "        for j in range(len(total_centroids[i])):\n",
    "            xval_cent.append(fov_nums[i][0] - 2000 + total_centroids[i][j][0])\n",
    "            yval_cent.append(fov_nums[i][1] - 2000 + total_centroids[i][j][1])\n",
    "    xval_positions.append(np.array(xval_cent))\n",
    "    yval_positions.append(np.array(yval_cent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude spots detected on the edge of imaging plane by making box of exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_exclusion_box(xvals, yvals, top, bottom, left, right):\n",
    "    return xvals[~((xvals > left) & (xvals < right) & (yvals > bottom) & (yvals < top))], yvals[~((xvals > left) & (xvals < right) & (yvals > bottom) & (yvals < top))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcoords = np.array([])\n",
    "for pos in xval_positions:\n",
    "    xcoords = np.append(xcoords, pos)\n",
    "ycoords = np.array([])\n",
    "for pos in yval_positions:\n",
    "    ycoords = np.append(ycoords, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_xval, try_yval = xcoords, ycoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9hr exclusion\n",
    "try_xval, try_yval = create_exclusion_box(xcoords, ycoords, top = 3000, bottom = 0, left = 0, right =5000)\n",
    "try_xval, try_yval = create_exclusion_box(try_xval, try_yval, top = 4000, bottom = 0, left = 0, right =100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter yes or no on candidate images as manual filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_spots_after_filtering = []\n",
    "scatters = np.unique(np.array([try_xval, try_yval]).T, axis = 0)\n",
    "for img in scatters:\n",
    "    iamge_play = bac_image_padded[int(img[0]-2000):int(img[0]+2000), int(img[1]-2000):int(img[1]+2000)]\n",
    "    dap_iamge_play = dapi_image_padded[int(img[0]-2000):int(img[0]+2000), int(img[1]-2000):int(img[1]+2000)]\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(iamge_play)\n",
    "    ax2.imshow(dap_iamge_play)\n",
    "    plt.show()\n",
    "    answer = input()\n",
    "    if answer == 'n':\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(os.path.join(path, 'final_spot_images'))\n",
    "        except:\n",
    "            None\n",
    "        plt.savefig(os.path.join(path, 'final_spot_images', str(int(img[0]))+'_'+str(int(img[1]))+'.png'))\n",
    "        final_spots_after_filtering.append((img[0], img[1]))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_final_spots = []\n",
    "for first in final_spots_after_filtering:\n",
    "    for second in range(len(xval_positions)):\n",
    "        if (first[0] in xval_positions[second]) and (first[1] in yval_positions[second]):\n",
    "            centroids_final_spots.append((first, second))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(path, 'centroids_final_spots.npy'),centroids_final_spots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Resolution Quantification and Filtering. Tune Hyperparameters accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from re import S\n",
    "import math\n",
    "class Found(Exception): pass\n",
    "hull_volumes = []\n",
    "hull_points = []\n",
    "new_centroids_final_spots = []\n",
    "for img in centroids_final_spots:\n",
    "        window = 300\n",
    "        iamge_play = bac_image_padded[int(img[0][0]-window):int(img[0][0]+window), int(img[0][1]-window):int(img[0][1]+window)]\n",
    "        hrpl_iamge_play = hrpl_image_padded[int(img[0][0]-window):int(img[0][0]+window), int(img[0][1]-window):int(img[0][1]+window)]\n",
    "        pvds_iamge_play = pvds_image_padded[int(img[0][0]-window):int(img[0][0]+window), int(img[0][1]-window):int(img[0][1]+window)]\n",
    "        spots, threshold = detection.detect_spots(  \n",
    "                images=iamge_play, \n",
    "                threshold=90,\n",
    "                return_threshold=True, \n",
    "                log_kernel_size=(1.456, 1.456),\n",
    "                minimum_distance=(1.456, 1.456))\n",
    "        real_spots, threshold_bac = detection.detect_spots(  \n",
    "                images=iamge_play, \n",
    "                threshold=95,\n",
    "                return_threshold=True, \n",
    "                spot_radius=(17, 17),\n",
    "                voxel_size = (3, 3))   \n",
    "        if len(real_spots) == 0:\n",
    "                real_spots = [[150, 150]]\n",
    "        spots_hrpl, threshold_sid = detection.detect_spots(  \n",
    "                images=hrpl_iamge_play, \n",
    "                return_threshold=True,\n",
    "                threshold=200,\n",
    "                spot_radius=(6, 4),\n",
    "                voxel_size = (3, 3))   \n",
    "\n",
    "        spots_pvds, threshold_sid = detection.detect_spots(  \n",
    "                images=pvds_iamge_play, \n",
    "                return_threshold=True,\n",
    "                threshold=115,\n",
    "                spot_radius=(6, 6),\n",
    "                voxel_size = (3, 3)) \n",
    "\n",
    "        total_spots_second = list(spots)\n",
    "        type_spots = ['bac' for i in range(len(spots))]\n",
    "        for i in spots_pvds:\n",
    "                total_spots_second.append(i)\n",
    "                type_spots.append('pvds')\n",
    "        for i in spots_hrpl:\n",
    "                total_spots_second.append(i)\n",
    "                type_spots.append('hrpl')\n",
    "        for i in real_spots:\n",
    "                total_spots_second.append(i)\n",
    "                type_spots.append('bac_whole')\n",
    "\n",
    "        total_spots_second = np.array(total_spots_second)\n",
    "        \n",
    "        try_samples = [30, 10, 5, 3]\n",
    "        use_cluster = []\n",
    "        ct_try = 0\n",
    "        try:\n",
    "                while True:\n",
    "                        clustering_second = DBSCAN(eps = 100, min_samples=try_samples[ct_try]).fit(total_spots_second)\n",
    "                        ct_try += 1\n",
    "                        for uniq in np.unique(clustering_second.labels_):\n",
    "                            if not ((len(np.unique(clustering_second.labels_)) == 1) and (-1 in clustering_second.labels_)):\n",
    "                                if uniq != -1:\n",
    "                                    point_labels = np.array([np.array(type_spots)[f] for f in np.where(clustering_second.labels_ == uniq)]).squeeze()\n",
    "                                    #print(len([i for i in point_labels if i == 'bac']))\n",
    "                                    try:\n",
    "                                            len([i for i in point_labels if i == 'bac'])\n",
    "                                    except:\n",
    "                                            None\n",
    "                                    if len([i for i in point_labels if i == 'bac']) < 3:\n",
    "                                            None\n",
    "                                    else:\n",
    "                                            use_cluster.append(uniq)\n",
    "                                if len(use_cluster) > 0:\n",
    "                                        raise Found\n",
    "        except Found:\n",
    "                print('found')\n",
    "\n",
    "        \n",
    "\n",
    "        hull_list = []\n",
    "        point_list = []\n",
    "        point_labels = []\n",
    "        for uniq in use_cluster:\n",
    "                if uniq != -1:\n",
    "                        points = np.array([total_spots_second[f] for f in np.where(clustering_second.labels_ == uniq)]).squeeze()\n",
    "                        points = np.array([np.array([i[1], i[0]]) for i in points])\n",
    "                        point_labels.append(np.array([np.array(type_spots)[f] for f in np.where(clustering_second.labels_ == uniq)]).squeeze())\n",
    "                        hull = ConvexHull(points)\n",
    "                        point_list.append(points)\n",
    "                        hull_list.append(hull)\n",
    "        from_center = []\n",
    "        for j in point_list:\n",
    "                average_points = np.mean(j, axis = 0)\n",
    "                distance_po = math.sqrt(((average_points[1] - 150)**2)+((average_points[0] - 150)**2))\n",
    "                from_center.append(distance_po)\n",
    "        try:\n",
    "                hull = hull_list[np.where(from_center == np.min(from_center))[0][0]]\n",
    "                points = point_list[np.where(from_center == np.min(from_center))[0][0]]\n",
    "                point_labels = point_labels[np.where(from_center == np.min(from_center))[0][0]]\n",
    "        except:\n",
    "                print(use_cluster)\n",
    "                print(point_list)\n",
    "                continue\n",
    "\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (10, 10), dpi = 200)\n",
    "\n",
    "        \n",
    "        ax1.imshow(iamge_play)\n",
    "        ax1.scatter([i[1] for i in real_spots], [i[0] for i in real_spots], c='red', s=1, label = 'Bacterial Genes')\n",
    "        \n",
    "        \n",
    "        ax1.set_title('Bacterial Colony Identification')\n",
    "        ax1.legend(loc=2, prop={'size': 6})\n",
    "        #ax1.plot(points[:,0], points[:,1], 'o')\n",
    "        for simplex in hull.simplices:\n",
    "                ax1.plot(points[simplex, 0], points[simplex, 1], 'w')\n",
    "        hull_volumes.append(hull.volume)\n",
    "        hull_points.append((points, total_spots_second, type_spots))\n",
    "        ax1.plot(points[hull.vertices,0], points[hull.vertices,1], 'w', lw=0.5)\n",
    "        #ax1.plot(points[hull.vertices[0],0], points[hull.vertices[0],1], 'ro', lw= 0.5)\n",
    "        ax2.imshow(hrpl_iamge_play)\n",
    "        ax2.scatter([i[1] for i in spots_hrpl], [i[0] for i in spots_hrpl], c='red', s=1, label = 'hrpL')\n",
    "        ax2.set_title('hrpL Raw')\n",
    "        ax3.imshow(pvds_iamge_play)\n",
    "        ax3.scatter([i[1] for i in spots_pvds], [i[0] for i in spots_pvds], c='red', s=1, label = 'pvds')\n",
    "        ax3.set_title('pvds Raw')\n",
    "        fig1 = plt.gcf()\n",
    "        plt.show()\n",
    "        keep = input()\n",
    "        try:\n",
    "                os.mkdir(os.path.join(path, 'final_spot_for_adata'))\n",
    "                os.mkdir(os.path.join(path, 'final_spot_for_adata','final_spot_images'))\n",
    "                os.mkdir(os.path.join(path, 'final_spot_for_adata', 'point_labels'))\n",
    "                os.mkdir(os.path.join(path, 'final_spot_for_adata', 'points'))\n",
    "                os.mkdir(os.path.join(path, 'final_spot_for_adata', 'hulls'))\n",
    "        except:\n",
    "                None\n",
    "        if keep == 'y':\n",
    "                fig1.savefig(os.path.join(path, 'final_spot_for_adata','final_spot_images', 'Bac_cluster_'+str(int(img[0][0]))+'_'+str(int(img[0][1]))+'.png'))\n",
    "                np.save(os.path.join(path, 'final_spot_for_adata','point_labels', 'Bac_cluster_point_labels_'+str(int(img[0][0]))+'_'+str(int(img[0][1]))+'.npy'), point_labels)\n",
    "                np.save(os.path.join(path, 'final_spot_for_adata','points', 'Bac_cluster_points_'+str(int(img[0][0]))+'_'+str(int(img[0][1]))+'.npy'), points)\n",
    "                file = open(os.path.join(path, 'final_spot_for_adata','hulls', 'Bac_cluster_hulls_'+str(int(img[0][0]))+'_'+str(int(img[0][1]))+'.pkl'), 'wb')\n",
    "                # dump information to that file\n",
    "                pickle.dump(hull, file)\n",
    "                file.close()        \n",
    "                new_centroids_final_spots.append(img)\n",
    "                \n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_adata, median_distance_from_centroid, number_hrpl = [], [], []\n",
    "hrpl_median_distance_from_centroid, number_pvds, pvds_median_distance_from_centroid = [], [], []\n",
    "total_spots_in_colony, hull_area = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to microns from pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transform_to_microns\n",
    "mer_images = r'D:\\Tatsuya\\merscope\\202210221316_avrrpt2-9h-rep1_VMSC01101\\out\\images'\n",
    "transformer = transform_to_microns.MerfishTransform(os.path.join(mer_images, 'micron_to_mosaic_pixel_transform.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate statistics about each colony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_hull = glob.glob(os.path.join(path, 'final_spot_for_adata', 'points', '*'))\n",
    "for i in files_hull:\n",
    "    point_array = np.load(i)\n",
    "    point_labels_array = np.load(i.replace('points', 'point_labels'))\n",
    "    total_spots_in_colony.append(len([j for j in point_labels_array if j != 'bac']))\n",
    "\n",
    "    average_points = np.mean(point_array, axis = 0)\n",
    "\n",
    "    avg_array = []\n",
    "    for j in point_array:\n",
    "        distance_po = math.sqrt(((average_points[1] - j[1])**2)+((average_points[0] - j[0])**2))\n",
    "        avg_array.append(distance_po)\n",
    "    median_distance_from_centroid.append(np.mean(avg_array))\n",
    "    \n",
    "    with open(i.replace('points', 'hulls').replace('.npy', '.pkl'), 'rb') as f:\n",
    "        huller = pickle.load(f)\n",
    "    f.close()\n",
    "    hull_area.append(huller.volume)\n",
    "\n",
    "    number_hrpl.append(len([j for j in point_labels_array if j == 'hrpl']))\n",
    "    number_pvds.append(len([j for j in point_labels_array if j == 'pvds']))\n",
    "\n",
    "    avg_array = []\n",
    "    for j in range(len(point_array)):\n",
    "        if point_labels_array[j] == 'hrpl':\n",
    "            distance_po = math.sqrt(((average_points[1] - point_array[j][1])**2)+((average_points[0] - point_array[j][0])**2))\n",
    "            avg_array.append(distance_po)\n",
    "    try:\n",
    "        hrpl_median_distance_from_centroid.append(np.mean(avg_array))\n",
    "    except:\n",
    "        hrpl_median_distance_from_centroid.append(0)\n",
    "    \n",
    "    avg_array = []\n",
    "    for j in range(len(point_array)):\n",
    "        if point_labels_array[j] == 'pvds':\n",
    "            distance_po = math.sqrt(((average_points[1] - point_array[j][1])**2)+((average_points[0] - point_array[j][0])**2))\n",
    "            avg_array.append(distance_po)\n",
    "    try:\n",
    "        pvds_median_distance_from_centroid.append(np.mean(avg_array))\n",
    "    except:\n",
    "        pvds_median_distance_from_centroid.append(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_adata = new_centroids_final_spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = np.array([[cen[0][0] for cen in centroid_adata],[cen[0][1] for cen in centroid_adata]]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r'D:\\Alex\\MERSCOPE_reanalysis_output\\troubleshooting\\out.npy', new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transform_to_microns\n",
    "mer_images = r'D:\\Tatsuya\\merscope\\202210221316_avrrpt2-9h-rep1_VMSC01101\\out\\images'\n",
    "\n",
    "transformer = transform_to_microns.MerfishTransform(os.path.join(mer_images,'micron_to_mosaic_pixel_transform.csv'))\n",
    "np.save(r'D:\\Alex\\MERSCOPE_reanalysis_output\\troubleshooting\\transformed.npy', transformer.pixel_to_micron_transform(np.load(r'D:\\Alex\\MERSCOPE_reanalysis_output\\troubleshooting\\out.npy')))\n",
    "coordinates_bacteria_spatial = np.load(r'D:\\Alex\\MERSCOPE_reanalysis_output\\troubleshooting\\transformed.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create final bacterial adata and save final positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_adata = coordinates_bacteria_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = pd.DataFrame(zip(median_distance_from_centroid, number_hrpl, hrpl_median_distance_from_centroid, number_pvds, pvds_median_distance_from_centroid, total_spots_in_colony, hull_area), columns = 'median_distance_from_centroid, number_hrpl, hrpl_median_distance_from_centroid, number_pvds, pvds_median_distance_from_centroid, total_spots_in_colony, hull_area'.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations.to_csv(os.path.join(output_folder, 'observations.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_folder, 'centroids_final_spots.npy'), centroid_adata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4335511914a41419d8c6e6f94849709432ec6fe90822f07d32ca9860b2d24dff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
